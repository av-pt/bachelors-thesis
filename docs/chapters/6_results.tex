\chapter{Results}\label{results}
% Introduction: PAN20 uses these measures, so we do, too.
For the evaluation of our approaches we use several traditional as well as recently proposed measures.
\textcolor{olive}{(textbook citation needed for following definitions)} We use the convention that a pair is in the positive class iff both texts are written by the same author.
$tp$, $tn$, $fp$, $fn$ stand for the number of cases that where classified correctly as positive (true positives), correctly as negative (true negatives), falsely as positive (false positives) and falsely as negative (false negatives) respectively.\\
% Explain OOF cross-validation
% Precision
The \textbf{precision} $pre = \frac{tp}{tp+fp}$ of a classifier is the percentage of correct positive classifications $tp$ over all classifications $tp+fp$.
Thus, a precision approaching 1 indicates that an AV classifier's same-author predictions are near fully correct, meaning that there are nearly no false positives.\\
% Recall
The \textbf{recall} $rec = \frac{tp}{tp+fn}$ of a classifier is the percentage of correctly classified positive samples $tp$ over all positive samples $tp+fn$.
The lower the recall, the fewer same-author cases are recognized and predicted as such by an AV classifier.
In turn, a recall of 1 indicates that all same-author cases have been correctly identified.\\
% F1
Ideally, we want a system that classifies all same-author cases and only those as positive.
To measure this behavior, the \textbf{F1-score} $F_1 = 2\cdot\frac{prec\cdot{}rec}{prec+rec}$ can be used.
If both, precision and recall, approach 1 the F1-score of an AV classifier also approaches its maximum of 1.
Note that the F1 score weights precision and recall equally.
Especially for forensic Authorship Verification applications though, a high precision is more important than a high recall, as same-author decisions might be used as evidence and therefore must be reliable.
Also, in our setup, the F1 score ignores true negatives and therefore does not give an insight into how well the classifier detects different-author cases correctly.
For this, the different-author class would need to be assigned the positive label.\\
To mitigate some of the problems of the measures above and to better asses AV classifier performance, we use two more recently introduced measures.
% C@1, Point: non-answers (0.5) are possible
To measure include same-author and different-author classifications in the evaluation, one could use the accuracy $acc = \frac{tp+tn}{n}$, where $n = tp+tn+fp+fn$ is the total number of cases.
However, as \cite{bevendorff2019unmaskingShortTexts} points out, the results are often uncertain.
Also, in real-world applications wrong answers might be worse than non-answers.
Therefore, to give classification systems the option to withhold answers for difficult-to-decide cases, we use the \textbf{c@1-score} introduced by \cite{penas2011c_at_1} and adopted by PAN.
It is defined as $c@1 = \frac{n_{ac}}{n}+\frac{n_{ac}}{n}\cdot{}\frac{n_u}{n} = \frac{1}{n}\cdot{}\left(n_{ac}+\frac{n_{ac}}{n}\cdot{}n_u\right)$ where $n$ is again the total number of cases, $n_{ac} = tp+tn$ is the number of accurately classified cases and $n_u$ is the number of undecided cases.
This way, undecided cases count towards the c@1-score as if they were answered with the accuracy of the decided cases.
When an AV classifier gives an answer to all cases, the C@1-score is reduced to the accuracy.
A system that leaves all cases unanswered, receives a score of 0.\\
% F0.5u
Lastly, we use the \textbf{F0.5u-score} introduced by \cite{bevendorff2019unmaskingShortTexts}.
It is defined as $F_{0.5u} = \frac{(1+0.5^2)\cdot{}n_{tp}}{(1+0.5^2)\cdot{}n_{tp}+0.5^2\cdot{}(n_{fn}+n_u)+n_{fp}}$.
As mentioned above, a high precision result is more robust than a high recall one.
To account for this, the F0.5u-score weights precision two times as much as recall.
In addition, it also allows the classifiers to give non-answers.
However, as unanswered cases are often not useful in real-world applications, it interprets them as wrong answers.
Thus, the F0.5u-score highly emphasizes on the precision of an AV classifier.\\

% PAN20 measures:     AUC, C@1, F0.5u, F1, overall
% Unmasking measures: Precision, Recall, C@1, F0.5u


The results of our experiments can be seen in figures \ref{tab:p_unmasking_gb}, \ref{tab:p_teahan_ff} and \ref{tab:p_teahan_gb}.
The tables show the absolute values of the transcription systems and their statistical significance compared to verbatim text in the first row.
The thresholds for statistical significance are:
\begin{itemize}
    \item $*$: statistically significant ($p < 0.05$)
    \item $*\! *$: very statistically significant ($p < 0.01$)
    \item $*\! *\! *$: highly statistically significant ($p < 0.001$)
\end{itemize}
$+$ marks an increase and $-$ marks a decrease compared to the top row.
For example, $0.5936^{*\! *}_{-}$ indicates very statistically significant decrease of the current transcription's performance compared to verbatim text.


% First: Unmasking
% ->
%- For GB Unmasking: Argue 32 runs for stable results. This still only creates 262 curves. Show which parameters where used for those 32 runs
% - still variance is high, needs about 9%? to be significant
Before analyzing the final results, let us take a look at the sets of the degradation curves that resulted from Unmasking
% 4 gram curves fall much quicker! Show them! -> Rather run again with better window.
% As tehere are no non-answers, f05u is only precision-enhanced, and c@1 prob also has changes

% Then: Teahan Compression
Increasing this interval improves precision and recall, because $non$-$answers$ are omitted in their calculation and thus only those answers are counted that the classifier is more certain of.
On the other hand, $F0.5u$ deteriorates as it counts $non$-$answers$ as wrong classifications.
Decreasing the uncertainty interval leads to a general decrease in performance as uncertain answers are counted  (they are binarized).

% FF
%As cleaning step is phonetically informed we'll use verbatimorignal for comparing.

%For r values higher than 0.05, prec rec higher, f05u cat1 lower.
%Throwing out cases, prec rec f1 only consider cases where classifier is more sure about the result.


% GB
%For CV, CV 4-grams, and Dolgo 4-grams the dataset is too small and all samples are predicted to be false, thus, Precision and Recall are ill-defined (zero division) and we'll leave these out.
%Removed because ill-defined: 'cv', 'cv_4grams', 'dolgo_4grams', 'punct_4grams', 'dolgo', 'asjp_4grams', 'refsoundex', 'soundex'

% Also: Remember to answer the questions from the Introduction


% Compare to other approaches from PAN20
%As indicated earlier, through cross validation, we can use the entire data set for training and for testing, thus milking it in the most effective way. hehe, correction lolol.
% Note Gutenberg dataset is really small


In the following, we will discuss a number of possible reasons for the overall negative trend that phonetic transcriptions bring to the results.\\
Converting graphemes to phonemes, in our case verbatim text to its $IPA$ transcription, is a difficult task.
Moreover, when transcribing automatically, the transcription algorithm does not have any information about the pronunciation of the speaker of a given text.
Thus, usually text is transcribed to either the General (North) American pronunciation or the Received Pronunciation\footnote{British English}.
We use g2pE by \cite{kyubyong2019g2pE} which employs the Carnegie Mellon University Pronouncing Dictionary to look up transcriptions for words.
The CMU dictionary uses North American English as its pronunciation standard.
Thus, by transcribing we assume that the author has a North American English phonetic preference.
Transcribing, for example, both an Irish author's and a Nigerian author's texts to American English, one can imagine that a lot of phonetically relevant information, that could be used to distinguish them, is lost.
In the same vein, authors might \textit{actively} make efforts to impart certain phonetic qualities into their texts that are based on topic rather than the author's unconscious phonetic preference.
This becomes especially clear when an author uses direct speech, which often occurs in the datasets we use as they are both based on sets of fictional stories.
The ``voice in the author's head'' when writing a direct speech passage presumably varies greatly depending on the traits of the character depicted in the story.
Thus, extracting these features might aid in topic or genre identification more so than in Authorship Verification.
To mitigate this, direct speech could be removed from text entirely.\\
Another limitation of phonetic transcriptions for Authorship Analysis is due to the low-level nature they work at.
An authors' freedom of self-expression is limited on the sub-segmental level, i.e., concerning individual sounds.
Usually, the meaning of a word changes together with its pronunciation.
The only words for which this is not the case are synonyms such as the words "begin" and "start".
Thus, authors that want to express similar ideas arguably will sound similar on the sub-segmental level not because of their phonetic preference but due to the proximity of the topics.
More importantly, if authors want to express different ideas the resulting transcriptions will also be different, without the Authorship Verification classifier knowing if this difference stems from two unique authors with varied phonetic preferences or from one author discussing different topics.
Supra-segmental features such as stress or prose can be utilized more freely and might give a more informative base for Authorship Analysis.\\
Lastly and perhaps most importantly, standard Unmasking without using $n$-grams works on the lexical level, i.e., it uses entire words as atomic units and ignores the information encoded by the symbols inside a word.
With the step of transcription, however, we are precisely attempting to enhance this inner-word information.
Thus, for Unmasking the transcription of data serves only as a phonetically-informed binning method for types.
To mitigate this, we tried to use an $n$-gram based transcription method, with XXX results.\\



Describe n-gram effects of pt!


% Problem 3 / Following thoughts: Phonetic transcriptions / features are probably bad Authorship predictors but better predictors for other features such as genre, topic, age of setting.
\begin{table}
\caption{Significance of changes for Unmasking using the Gutenberg dataset, bonferroni-corrected.}
\label{tab:p_unmasking_gb}
\centering\small
\begin{tabular}{@{}l@{\hspace{1\tabcolsep}}lllll@{}} % Use @{\hspace{2\tabcolsep}} to double the spacing
\toprule
\bf System & \bf Precision & \bf Recall & \bf F1 & \bf F0.5u & \bf c@1 \\
\midrule
$Verbatim$ & $0.7545$ & $0.6539$ & $0.6977$ & $0.6784$ & $0.6875$ \\
\midrule
$IPA$ & $0.7537$ & $0.6701$ & $0.7004$ & $0.6641$ & $0.6833$ \\
$ASJP$ & $0.7284$ & $0.667$ & $0.6893$ & $0.6455$ & $0.676$ \\
$Dolgo$ & $0.7551$ & $0.6733$ & $0.7046$ & $0.6617$ & $0.6792$ \\
$RefSoundex$ & $0.7297$ & $0.6275$ & $0.6673$ & $0.6233^{*\! *}_{-}$ & $0.6542$ \\
$Metaphone$ & $0.7193$ & $0.6028$ & $0.648^{*}_{-}$ & $0.6143^{*\! *}_{-}$ & $0.6423^{*\! *}_{-}$ \\
$Soundex$ & $0.7333$ & $0.6363$ & $0.6677$ & $0.6234^{*\! *}_{-}$ & $0.6512$ \\
$CV$ & $0.6316^{*\! *}_{-}$ & $0.5757$ & $0.5936^{*\! *}_{-}$ & $0.5171^{*\! *\! *}_{-}$ & $0.5381^{*\! *\! *}_{-}$ \\
$IPA$ $4$-$grams$ & $0.6536^{*\! *}_{-}$ & $0.5814$ & $0.6043^{*\! *}_{-}$ & $0.5505^{*\! *\! *}_{-}$ & $0.5976^{*\! *\! *}_{-}$ \\
$ASJP$ $4$-$grams$ & $0.721$ & $0.6148$ & $0.6553$ & $0.6086$ & $0.6257$ \\
$P$ $4$-$grams$ & $0.718$ & $0.6339$ & $0.6658$ & $0.6207^{*}_{-}$ & $0.6387^{*}_{-}$ \\
$Dolgo$ $4$-$grams$ & $0.6894$ & $0.5964$ & $0.6245$ & $0.5558^{*\! *\! *}_{-}$ & $0.6012^{*\! *}_{-}$ \\
$CV$ $4$-$grams$ & $0.6161^{*\! *}_{-}$ & $0.5853$ & $0.5926^{*\! *}_{-}$ & $0.4868^{*\! *\! *}_{-}$ & $0.5151^{*\! *\! *}_{-}$ \\
$P$ & $0.7155$ & $0.6672$ & $0.6859$ & $0.6452$ & $0.6745$ \\
$PL$ & $0.7707$ & $0.6801$ & $0.7153$ & $0.6876$ & $0.7079$ \\
$PLS$ & $0.7676$ & $0.6614$ & $0.7031$ & $0.6572$ & $0.6905$ \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}
\caption{Significance of changes for the compression approach using the Fan-fiction dataset compared to verbatim text (uncleaned), $r=0.05$, bonferroni-corrected.}
\label{tab:p_teahan_ff}
\centering\small
\begin{tabular}{@{}l@{\hspace{1\tabcolsep}}lllll@{}} % Use @{\hspace{2\tabcolsep}} to double the spacing
\toprule
\bf System & \bf Precision & \bf Recall & \bf F1 & \bf F0.5u & \bf c@1 \\
\midrule
$Verbatim$ $(orig.)$ & $0.7635$ & $0.8092$ & $0.7856$ & $0.714$ & $0.7431$ \\
\midrule
$IPA$ & $0.7532^{*\! *\! *}_{-}$ & $0.7846^{*\! *\! *}_{-}$ & $0.7686^{*\! *\! *}_{-}$ & $0.7098^{*\! *\! *}_{-}$ & $0.7291^{*\! *\! *}_{-}$ \\
$Verbatim$ & $0.7604^{*\! *\! *}_{-}$ & $0.8078^{*\! *}_{-}$ & $0.7833^{*\! *\! *}_{-}$ & $0.7101^{*\! *\! *}_{-}$ & $0.739^{*\! *\! *}_{-}$ \\
$ASJP$ & $0.7602^{*\! *}_{-}$ & $0.7857^{*\! *\! *}_{-}$ & $0.7727^{*\! *\! *}_{-}$ & $0.7148$ & $0.7353^{*\! *\! *}_{-}$ \\
$Dolgo$ & $0.7474^{*\! *\! *}_{-}$ & $0.7757^{*\! *\! *}_{-}$ & $0.7612^{*\! *\! *}_{-}$ & $0.6992^{*\! *\! *}_{-}$ & $0.7174^{*\! *\! *}_{-}$ \\
$RefSoundex$ & $0.7564^{*\! *\! *}_{-}$ & $0.7811^{*\! *\! *}_{-}$ & $0.7685^{*\! *\! *}_{-}$ & $0.7049^{*\! *\! *}_{-}$ & $0.7259^{*\! *\! *}_{-}$ \\
$Metaphone$ & $0.772^{*\! *\! *}_{+}$ & $0.7907^{*\! *\! *}_{-}$ & $0.7813^{*\! *\! *}_{-}$ & $0.7266^{*\! *\! *}_{+}$ & $0.7477^{*\! *\! *}_{+}$ \\
$Soundex$ & $0.7129^{*\! *\! *}_{-}$ & $0.7717^{*\! *\! *}_{-}$ & $0.7411^{*\! *\! *}_{-}$ & $0.6758^{*\! *\! *}_{-}$ & $0.6839^{*\! *\! *}_{-}$ \\
$CV$ & $0.6371^{*\! *\! *}_{-}$ & $0.8172^{*\! *}_{+}$ & $0.716^{*\! *\! *}_{-}$ & $0.6253^{*\! *\! *}_{-}$ & $0.5842^{*\! *\! *}_{-}$ \\
$P$ & $0.7528^{*\! *\! *}_{-}$ & $0.7914^{*\! *\! *}_{-}$ & $0.7716^{*\! *\! *}_{-}$ & $0.7092^{*\! *\! *}_{-}$ & $0.7304^{*\! *\! *}_{-}$ \\
$PL$ & $0.7228^{*\! *\! *}_{-}$ & $0.7868^{*\! *\! *}_{-}$ & $0.7534^{*\! *\! *}_{-}$ & $0.6782^{*\! *\! *}_{-}$ & $0.6949^{*\! *\! *}_{-}$ \\
$PLS$ & $0.6994^{*\! *\! *}_{-}$ & $0.7773^{*\! *\! *}_{-}$ & $0.7363^{*\! *\! *}_{-}$ & $0.6604^{*\! *\! *}_{-}$ & $0.668^{*\! *\! *}_{-}$ \\
\bottomrule
\end{tabular}
\end{table}


\begin{table}
\caption{Significance of changes for the compression approach using the Gutenberg dataset compared to verbatim text, $r=0.05$, bonferroni-corrected.}
\label{tab:p_teahan_gb}
\centering\small
\begin{tabular}{@{}l@{\hspace{1\tabcolsep}}lllll@{}} % Use @{\hspace{2\tabcolsep}} to double the spacing
\toprule
\bf System & \bf Precision & \bf Recall & \bf F1 & \bf F0.5u & \bf c@1 \\
\midrule
$Verbatim$ & $0.871$ & $0.7354$ & $0.7909$ & $0.6163$ & $0.645$ \\
\midrule
$IPA$ & $0.8545$ & $0.676^{*}_{-}$ & $0.7383^{*}_{-}$ & $0.5951$ & $0.5396^{*\! *\! *}_{-}$ \\
$ASJP$ & $0.8738$ & $0.7241$ & $0.7786$ & $0.607$ & $0.5325^{*\! *\! *}_{-}$ \\
$Metaphone$ & $0.9441^{*\! *}_{+}$ & $0.7265$ & $0.8005$ & $0.5824^{*\! *}_{-}$ & $0.4088^{*\! *\! *}_{-}$ \\
$IPA$ $4$-$grams$ & $0.9117$ & $0.74$ & $0.7895$ & $0.5611^{*\! *\! *}_{-}$ & $0.3793^{*\! *\! *}_{-}$ \\
$P$ & $0.8816$ & $0.7178$ & $0.7801$ & $0.5979$ & $0.5655^{*\! *\! *}_{-}$ \\
$PL$ & $0.8664$ & $0.7237$ & $0.7734$ & $0.592^{*\! *}_{-}$ & $0.5554^{*\! *\! *}_{-}$ \\
$PLS$ & $0.8006$ & $0.7056$ & $0.737^{*}_{-}$ & $0.6165$ & $0.6197$ \\
\bottomrule
\end{tabular}
\end{table}