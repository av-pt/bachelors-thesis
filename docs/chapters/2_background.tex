\chapter{Background}\label{ch:background}
% Authorship Verification
\section{Authorship Verification}\label{sec:authorship-verification}
In our world a large amount of communication and information transfer is done through a visual medium: text.
This was not always the case.
According to \cite{owidliteracy}, for most of history, reading and writing was reserved to an elite and associated with power.
Through the spread of public education over time, increasingly more people were able to read and write.
World-wide literacy rates are estimated to have been at 12\% in 1820 and have risen to 86\% in 2016.
\cite{buringh2009charting} estimates that the number of books produced annually in Western Europe in the sixth and seventh centuries was only around 120.
In contrast, and not only due to the invention of letterpress printing with movable type, the production in 1790 was at a peak of 20,000,000 books per year.
These historical processes gave rise to the linguistic branch of Stylometry, the analysis of "an author's work based especially on the recurrence of particular turns of expression or trends of thought"\footnote{\url{https://www.merriam-webster.com/dictionary/stylometry}}.
The worth of profiling authors can be exemplified with a court case from 1979 \textcolor{olive}{(citation needed, this is from wikipedia)}.
The American linguist Roger Shuy was able to infer that the author of a ransom note linked to a kidnapping case had to be well-educated and from Akron, Ohio.
Using this information, the offender was caught and later confessed the crime.\\
With the adaption of computers by the linguistic community, stylometry was increasingly done automatically.
The wide availability of training data and the increasing speed of computers allowed for more involved and complex stylometric methods \textcolor{olive}{(citation needed [holmes1998])}.
Nowadays, Authorship Analysis, concerned mainly with determining authors of given texts, is consolidated as a subbranch of stylometry.
According to \cite{bevendorff2020shared}, it can be further parted into four disciplines:\\
\textit{Authorship Attribution} aims at selecting the actual author of a given text document from a list of candidate authors.
This task simulates, for example, a forensic situation where the document is a threatening letter, and it has to be determined which of the possible suspects created it.\\
\textit{Authorship Obfuscation} changes the direction of the effort.
Instead of trying to determine an author, the goal is to paraphrase a given text in such a way that its author cannot be identified by comparing it to other non-paraphrased texts of the same author.
For example, the author of said threatening letter could actively try to use different language to conceal their identity.
Although it seems at first counter-intuitive to model this behavior, by developing working obfuscation methods mistakes and pitfalls in the creation of Authorship Analysis methods can be uncovered.\\
To this end, \textit{Obfuscation Evaluation} measures are devised which assess the performance of the obfuscation methods.\\
Lastly, \textit{Authorship Verification}, which is the task of interest to our research, is defined as: "[G]iven a pair of documents, determine whether they are written by the same author".
According to \cite{stein2019unbiasedGutenbergCorpus}, this task was first proposed in 2004 by \cite{koppel2004unmasking}, and thus is a rather recent development.
Authorship Verification abstracts on other Authorship Analysis tasks by employing only basic inputs and outputs.
In contrast to Authorship Attribution, it does not aim to identify actual authors but only the fact of whether they are different or not.
This arguably makes Authorship Verification a more difficult task, since no additional information about the authors of the two texts can be used to extract author profiles from.
Instead, Authorship Verification algorithms try to identify the universal characteristics by which authors can be differentiated, regardless of the style of one specific author.
As stated in \cite{bevendorff2020shared}, the PAN series of shared tasks\footnote{\url{https://pan.webis.de/}}\textsuperscript{,}\footnote{Wikipedia: "originally, plagiarism analysis, authorship identification, and near-duplicate detection, later more generally workshop on uncovering plagiarism, authorship, and social software misuse", \url{https://en.wikipedia.org/wiki/Stylometry\#PAN}} included Authorship Verification tasks from 2013 to 2015 and picked them back up with PAN2020, with a planned total of three tasks over the years 2020 to 2022.
In the course of reintegrating the task to PAN, a new data format has been devised, which we will use in our research.\\
%Roughly based on the notation in \cite{bevendorff2020shared} the task can be formalized as follows.
Authorship Verification can be formalized as follows.
Given a text pair $(d_1, d_2)$, classify it to ${True, False}$, i.e.,\ approximate the target function $\phi{}:(d_1, d_2)\to\{True, False\}$ where $\phi(d_1, d_2)=True$ iff $d_1$ and $d_2$ have the same author.
We call an algorithm that approximates $\phi$ an Authorship Verification classifier, or AV classifier for short.
In practice, the output of a binary classifier is often a real number in the interval $[0,1]$, signifying the probability that a sample belongs to the positive or $True$ class.
As discussed later, the precision of an AV classifier is arguably more important than its recall.
Because of this, PAN2020 uses metrics that take non-answers into account.
An AV classifier can decide that the chance of a same-author classification is not high enough and withhold an answer.
Usually, this is implemented with an uncertainty interval around $0.5$ for which $non\text{-}answer$ is returned.


% Phonetics
\section{Phonetic Transcriptions}\label{sec:phonetic_transcriptions}
As defined in \cite{ogrady2017introToLinguistics}, which serves as a reference for the notions in this section, phonetics\footnote{Merriam-Webster: "borrowed from New Latin ph$\bar{\mbox{o}}$n$\bar{\mbox{e}}$ticus "(of written characters) representing speech sounds rather than ideas,"[...]", \url{https://www.merriam-webster.com/dictionary/phonetics}. \textcolor{teal}{(Etymology is more complicated. Should it be expanded?)}} is the branch of linguistics concerned with "the inventory and structure of the sounds of speech".
Not all sounds humans can articulate are present in the worlds languages.
Yet a wide range, estimated to consist of 600 consonant and 200 vowel sounds, occur in human language.
Note that phonetics is different from phonology, in that phonology examines how sounds create meaning in a language.
Two subbranches of phonetics are articulatory phonetics and acoustic phonetics.
The former concerns itself with the physiological processes involved in speech production while the latter examines acoustic characteristics of speech.
In our research we focus on articulatory phonetics.\\
The distinct sounds of which a spoken utterance is made up are called \textit{phones}.
On a more abstract level, linguists segment speech into \textit{phonemes}.
Phonemes are defined as the smallest unit of sound distinguishing meaning in the words of a language.
Swapping a phoneme in a word changes its meaning, while replacing a phone with a different one does not necessarily alter its meaning.
Those sets of phones that do not evoke a change of meaning when exchanged are called \textit{allophones} of their respective phonemes.
For example, the alveolar nasal consonant [n] and the dental nasal consonant [\textipa{\|[n}] are allophones of the phoneme /n/ as they are not used to differentiate meaning in English -- [\textipa{w2n}] and [\textipa{w2\|[n}] both point to the same concept "one" \textcolor{teal}{(Do I need to explain [] notation? Is // ok here for phoneme notation?)}.
However, the alveolar nasal consonant [n] and the bilabial nasal consonant [m] are different, contrasting phonemes -- [\textipa{m\ae{}p}] and [\textipa{n\ae{}p}] indicate the two distinct concepts "map" and "nap".
While phones are universal, phonemes are language specific \textcolor{olive}{(citation needed)}.\\
XXX GO IN DEPTH ABOUT PHONETIC PREFERENCE
As hypothesized in the introduction, we suspect that information valuable for identifying authorship exists on the phonetic level.
Because Authorship Verification classifiers use text as input, we want to utilize methods to extract phonetically relevant features from these texts.
One possible way of achieving this is with phonetic transcriptions.
For our purposes, these are transformations assigning a symbol to each sound of a text as if the text was spoken aloud.
Phonetic transcriptions can be seen as data reduction methods.
By applying them, we anticipate that the phonetically relevant features stay apparent while other, less relevant features stand out less.
In total, we use seven phonetic transcription systems of different granularity.
The \textit{narrower} a transcription, the more closely it follows the phonetic details of an utterance.
This often leads to the system having a bigger alphabet, such as the IPA described below.
The \textit{broader} a transcription, the more it generalizes phonetic features.
Table \ref{tab:example_transcriptions} shows examples for the transcription systems we employ.\\

\begin{table}
\caption{Example transcriptions.}
\label{tab:example_transcriptions}
\centering\small
\begin{tabular}{@{}l@{\hspace{3\tabcolsep}}l@{}} % Use @{\hspace{2\tabcolsep}} to double the spacing
\toprule
\bf System & \bf Transcription \\
\midrule
Verbatim   & Wake and rise, and step into the green outdoors.\footnote{From \cite{ieee1969sentences}, Appendix C, List 58.5} \\
IPA        & \textipa{weIk 2nd \*raIz 2nd stEp Intu D2 g\*rin aUtdO\*rz} \\
Dolgo      & WVK VNT RVS VNT STVP VNTV TV KRVN VTTVRS \\
ASJP       & wek ond raz ond stEp intu 8o grin atdorz \\
CV         & CVC VCC CVC VCC CCVC VCCV CV CCVC VCCVCC \\
Soundex    & W200 A530 R200 A530 S310 I530 T000 G650 O362 \\
RefSoundex & W030 A086 R9030 A086 S3601 I0860 T60 G4908 O06093 \\
Metaphone  & WK ANT RS ANT STP INT 0 KRN OTTRS \\
\bottomrule
\end{tabular}
\end{table}

% IPA
The most widely used phonetic transcription system is the International Phonetic Alphabet (\textbf{IPA}). % Cite O'Grady?! I mean, this is fact...
As outlined in Handbook of the International Phonetic Association [1999], it was developed by the International Phonetic Association founded in 1886 \textcolor{teal}{(How to cite the Handbook of the International Phonetic Association? There is no author.)}.
It serves as a system to notate the sounds of languages in an internationally agreed-upon manner.
Pulmonic consonants - consonants initiated by a buildup of pressure from the lungs - are distinguished in their place and manner of articulation.
The place of articulation describes the position in the vocal tract where the sound is produced.
For example, a bilabial sound, such as the "b" in "beer", is articulated with both lips whereas a glottal sound, such as the "h" in "hello", is articulated all the way back at the glottis.  % Explain glottis in footnote?
The manner of articulation includes several factors regarding distinctive ways of sound production.
To give an example, a plosive, such as the "p" in "explosion", is created by completely stopping the airflow, building up pressure and suddenly releasing said pressure.
The IPA also differentiates between voiced and voiceless consonants such as the first phonemes in the words "vast" and "fast".
In a similar way, non-pulmonic consonants and vowels are organized on scales such as position and manner of articulation.
This way, a system to classify arbitrary sounds of a language has been created.
With 155 symbols, its alphabet is the largest of the transcription systems considered in this thesis.
Therefore, the produced transcriptions are usually the narrowest.
It should be noted that when using the IPA system a transcription can be much more detailed than just using the correct symbols for the phonemes.
Using diacritics, many other qualities of speech, such as the roundedness of the lips, can be indicated.
Creating accurate and detailed transcriptions of a given speech sample on the level of phones is a difficult task usually done manually by linguists.
This ties into a problem we have found in our research that we will discuss later on.
To achieve more stable results, we use a slightly broader version of the IPA omitting prosodic markers and diacritics. % Note exact size?
\phantom{\cite{ipa1999ipaHandbook}}\\


% Sound Classes
Because of its detailed nature, IPA transcriptions contain a lot of information.
Continuing with the idea of reducing phonetically irrelevant information, we also employ broader transcription systems.
The following ones can be categorized as sound class systems organizing speech sounds into linguistically-informed classes.\\
% Dolgo
According to \cite{list2012multiple}, the term sound class was first devised and detailed in \cite{dolgopolsky1986dolgoOriginal}.
For conciseness, we will use the term more generically as defined above.
In the Dolgopolsky sound class system (\textbf{Dolgo}), phonemes are organized into ten classes, so that the difference between sounds inside of a class is less than the difference between classes.
These classes are derived manually from empirical data.
% Based on empirical data, phonemes are organized into the classes, so that 'phonetic correspondences inside a "type" are more regular than between different types'.
\textcolor{teal}{(Info is from List paper above, a bit unclear if "correspondence relations" simply means "coocurrences (of phonemes)".)}
We use a slightly extended version of the original $Dolgo$ sound class system, as implemented in \cite{list2018cltsIntro}.
It includes an eleventh class for vowels and is compatible with all IPA symbols including common diacritics.
A list of the $Dolgo$ sound classes with examples for corresponding phonemes can be seen in \ref{tab:dolgo_sound_classes}.


\begin{table}
\caption{$Dolgo$ sound classes, adopted from \cite{list2010dolgoRefined} with the eleventh category "V" added.}
\label{tab:dolgo_sound_classes}
\centering\small
\begin{tabular}{@{}c@{\hspace{3\tabcolsep}}cc@{}} % Use @{\hspace{2\tabcolsep}} to double the spacing
\toprule
\bf Symbol & \bf Example phonemes (IPA) & \bf Category \\
\midrule
P & p, b, f                     & labial obstruents \\
T & d, t, \textipa{T, D}        & dental obstruents \\
S & s, z, \textipa{S, Z}        & sibilants \\
K & k, g, \textipa{ts, tS}      & velar obstruents, dental and alveolar affricates \\
M & m                           & labial nasal \\
N & n, \textipa{\textltailn, N} & remaining nasals \\
R & r, l                        & liquids \\
W & v, u                        & voiced labial fricative and initial rounded vowels \\
J & j                           & palatal approximant \\
H & h, \textipa{H, N}(repeat?)  & laryngeals and initial velar nasal \\
V & \textipa{A, E, I}           & other vowels (simple and diphtongs) \\
\bottomrule
\end{tabular}
\end{table}


% ASJP
The Automated Similarity Judgment Program is a project aiming to classify the world's languages introduced in \cite{brown2008asjpCode}.
As of June 26, 2021, it consists of a database comprising 40-word lists of core vocabulary translated to 9,788 languages.
The word lists include meanings such as "I", "drink", and "water".
Each word is transcribed using the asjpCode transcription system.
This way, phonetic similarities between language pairs can be computed.
Language-similarity-trees created with ASJP produce near expert-level classifications.
AsjpCode (\textbf{ASJP}) consists of 34 consonant and 7 vowel symbols.
It can be seen as a simplified variant of the IPA system, with some symbols representing a broader class of speech sounds.
For example, "N" represents the velar nasal [\textipa{N}] directly, while "o" represents all rounded and unrounded mid and low back vowels [\textipa{7, 2, A, o, O, 6}].
Another benefit of asjpCode, although not directly influential to our research, is that it consists of only those symbols which are found on a standard QWERTY keyboard.
This facilitates manual transcription.\\
% CV
Lastly, the \textbf{CV} sound class system assigns the symbol "C" to consonant phonemes and the symbol "V" to vowel phonemes as done in \cite{list2017lingpy}.
With a binary alphabet it is the broadest of the transcription systems we use.\\

% Soundex
Apart from these systems we also examine the impact of three simple phonetic algorithms.
These algorithms were invented to match words of similar pronunciation in English.
The \textbf{Soundex} algorithm, patented by Robert C. Russell in \cite{russel1918soundex} and \cite{russel1922soundex}, was devised for indexing names.
By grouping names by phonetic similarity instead of alphabetically, the time needed to search for a given name would be shortened.
Also, similar sounding names that are written differently would be organized into the same categories simplifying access when, for example, only a spoken name is given.
A word is represented by a code consisting of a capital letter, the first character of the word, and three digits.
The digits, ranging from 1 to 6, represent sound classes of letters occurring later in the word.
Table \ref{tab:soundex_sound_classes} shows these classes in more detail.
The process of assigning these codes roughly functions as follows.
The first letter in the word is used as the beginning letter of the code.
The first letter and all occurrences of the letters "a", "e", "i", "o", "u", "y", "h", and "w" are removed.
The remaining letters are encoded using the mapping from \ref{tab:soundex_sound_classes}.
If two equal sound classes appear next to each other, the second occurrence is removed.
The resulting code is truncated to a length of four characters in total.
If the code is shorter than four characters it is filled up with trailing zeros.
\textcolor{olive}{(citation needed: patent differentiates between m and n, algorithms do not (-> class 5))}\\

\begin{table}
\caption{Soundex sound classes as used in our research.}
\label{tab:soundex_sound_classes}
\centering\small
\begin{tabular}{@{}c@{\hspace{3\tabcolsep}}cc@{}} % Use @{\hspace{2\tabcolsep}} to double the spacing
\toprule
\bf Symbol & \bf Associated characters & \bf Category \\
\midrule
1 & b, f, p, v             & labials, labio-dentals \\
2 & c, g, j, k, q, s, x, z & gutturals, sibilants \\
3 & d, t                   & dental-mutes \\
4 & l                      & palatal-fricatives \\
5 & m, n                   & nasals \\
6 & r                      & dental-fricatives \\
\bottomrule
\end{tabular}
\end{table}

The Refined Soundex algorithm (\textbf{RefSoundex}) improves upon its predecessor, with the main difference being that the resulting codes are no longer truncated or extended to a length of 4 but instead retain their original length.
Also, the number of the sound classes is increased to nine, instead of six, leading to a narrower transcription.
The alternate mapping can be seen in \ref{tab:refsoundex_sound_classes}.
Lastly, the digit sequence following the first character also includes this character's sound class symbol.
The word "and", for example, is transcribed to "A086", not "A86".
\cite{howard2019refsoundexSource1} traces the origins of Refined Soundex back to an implementation in the Apache Commons Library as noted in \cite{fossati2008refsoundexSource2}, but indicates that the idea of modifying the sound classes already appeared in \cite{zobel1995refsoundexSource3}.\\

\begin{table}
\caption{Refined Soundex sound classes as used in our research.}
\label{tab:refsoundex_sound_classes}
\centering\small
\begin{tabular}{@{}c@{\hspace{3\tabcolsep}}cc@{}} % Use @{\hspace{2\tabcolsep}} to double the spacing
\toprule
\bf Symbol & \bf Associated characters & \bf Category \\
\midrule
0 & a, e, i, o, u, y, h, w & (vowel-like?) \\
1 & b, p                   & labials \\
2 & f, v                   & labio-dentals \\
3 & c, k, s                & (?) \\
1 & g, j                   & (gutturals?) \\
2 & q, x, z                & (?) \\
3 & d, t                   & dental-mutes \\
4 & l                      & palatal-fricatives \\
5 & m, n                   & nasals \\
6 & r                      & dental-fricatives \\
\bottomrule
\end{tabular}
\end{table}

% Metaphone
\textbf{Metaphone} is also a phonetic indexing algorithm first published in \cite{philips1990metaphone} \textcolor{olive}{(citation needed / paper not found)}.
It improves on the Soundex family of algorithms by taking a larger number of inconsistencies and edge-cases of English pronunciation into account.
Also its focus does not only lie on indexing names but rather English words in general.
It consists of a series of 27 context-aware transformations\footnote{Number of transformations stems from the implementation used.}, sequentially replacing phonetically similar patterns with representative symbols or removing them if they are not unpronounced.
For example, one such transformation is removing the first letter of a word if that word starts with "KN", "GN", "PN", "AE", or "WR".
$Metaphone$'s alphabet consists of only 21 symbols -- 16 for consonants and 5 for vowels -- representing classes of speech sounds.
Vowel symbols only appear at the beginning of transcribed words.
Metaphone was later superseded by Double Metaphone and the closed-source Metaphone 3, both of which use a substantially larger rule set.\\
SMt like: There are more systems but they are not well supported for automatic transcription etc.

For normalization, we remove all inter-word punctuation in the texts.
Intra-word punctuation, such as in "don't", is phonetically significant and thus not removed.
For brevity, we refer to all systems described above as phonetic transcription systems.
In addition to the phonetic transcriptions above, we also create three other conversions for comparison:
\begin{itemize}
  \item \textbf{P}: Removing punctuation
  \item \textbf{PL}: Removing punctuation and lemmatizing the occurring words
  \item \textbf{PLS}: Removing punctuation, lemmatizing, and removing stop words
\end{itemize}
We handle $Verbatim$ text and the three non-phonetic conversions the same way as transcribed text.




%\begin{table}
%\caption{Naive Approach}
%\label{tab:naive_approach}
%\centering\small
%\begin{tabular}{@{}l@{\hspace{3\tabcolsep}}llllll@{}} % Use @{\hspace{2\tabcolsep}} to double the spacing
%\toprule
%\bf Approach & \bf F1 & \bf AUC & \bf c@1 & \bf f0.5u & \bf precision & \bf recall \\
%\midrule
%Vocab size & 0.726 & 0.697  & 0.663 & 0.67 & 0.6368 & 0.8444\\
%Random     & 0.512 & 0.498  & 0.498 & 0.521 & 0.5272 & 0.4979\\
%\bottomrule
%\end{tabular}
%\end{table}

% vocab size "overall": 0.689
% random 'overall': 0.507


% Supra-segmental features
% Warum und Wie wird beides verbunden?
% Maybe put this in Introduction!
% Integrating phonetics into stylometric methods is not an entirely new endeavour...
% Hypothesis: Phonetic preference

