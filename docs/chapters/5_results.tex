\chapter{Results}\label{results}
% Introduction: PAN20 uses these measures, so we do, too.
For the evaluation of our approaches we use several traditional as well as recently proposed measures.
\textcolor{olive}{(textbook citation needed for following definitions)} We use the convention that a pair is in the positive class iff both texts are written by the same author.
$tp$, $tn$, $fp$, $fn$ stand for the number of cases that where classified correctly as positive (true positives), correctly as negative (true negatives), falsely as positive (false positives) and falsely as negative (false negatives) respectively.\\
% Explain OOF cross-validation
% Precision
The \textbf{precision} $pre = \frac{tp}{tp+fp}$ of a classifier is the percentage of correct positive classifications $tp$ over all classifications $tp+fp$.
Thus, a precision approaching 1 indicates that an AV classifier's same-author predictions are near fully correct, meaning that there are nearly no false positives.\\
% Recall
The \textbf{recall} $rec = \frac{tp}{tp+fn}$ of a classifier is the percentage of correctly classified positive samples $tp$ over all positive samples $tp+fn$.
The lower the recall, the fewer same-author cases are recognized and predicted as such by an AV classifier.
In turn, a recall of 1 indicates that all same-author cases have been correctly identified.\\
% F1
Ideally, we want a system that classifies all same-author cases and only those as positive.
To measure this behavior, the \textbf{F1-score} $F_1 = 2\cdot\frac{prec\cdot{}rec}{prec+rec}$ can be used.
If both, precision and recall, approach 1 the F1-score of an AV classifier also approaches its maximum of 1.
Note that the F1 score weights precision and recall equally.
Especially for forensic Authorship Verification applications though, a high precision is more important than a high recall, as same-author decisions might be used as evidence and therefore must be reliable.
Also, in our setup, the F1 score ignores true negatives and therefore does not give an insight into how well the classifier detects different-author cases correctly.
For this, the different-author class would need to be assigned the positive label.\\
To mitigate some of the problems of the measures above and to better asses AV classifier performance, we use two more recently introduced measures.
% C@1, Point: non-answers (0.5) are possible
To measure include same-author and different-author classifications in the evaluation, one could use the accuracy $acc = \frac{tp+tn}{n}$, where $n = tp+tn+fp+fn$ is the total number of cases.
However, as \cite{bevendorff2019unmaskingShortTexts} points out, the results are often uncertain.
Also, in real-world applications wrong answers might be worse than non-answers.
Therefore, to give classification systems the option to withhold answers for difficult-to-decide cases, we use the \textbf{c@1-score} introduced by \cite{penas2011c_at_1} and adopted by PAN.
It is defined as $c@1 = \frac{n_{ac}}{n}+\frac{n_{ac}}{n}\cdot{}\frac{n_u}{n} = \frac{1}{n}\cdot{}\left(n_{ac}+\frac{n_{ac}}{n}\cdot{}n_u\right)$ where $n$ is again the total number of cases, $n_{ac} = tp+tn$ is the number of accurately classified cases and $n_u$ is the number of undecided cases.
This way, undecided cases count towards the c@1-score as if they were answered with the accuracy of the decided cases.
When an AV classifier gives an answer to all cases, the C@1-score is reduced to the accuracy.
A system that leaves all cases unanswered, receives a score of 0.\\
% F0.5u
Lastly, we use the \textbf{F0.5u-score} introduced by \cite{bevendorff2019unmaskingShortTexts}.
It is defined as $F_{0.5u} = \frac{(1+0.5^2)\cdot{}n_{tp}}{(1+0.5^2)\cdot{}n_{tp}+0.5^2\cdot{}(n_{fn}+n_u)+n_{fp}}$.
As mentioned above, a high precision result is more robust than a high recall one.
To account for this, the F0.5u-score weights precision two times as much as recall.
In addition, it also allows the classifiers to give non-answers.
However, as unanswered cases are often not useful in real-world applications, it interprets them as wrong answers.
Thus, the F0.5u-score highly emphasizes on the precision of an AV classifier.\\

% PAN20 measures:     AUC, C@1, F0.5u, F1, overall
% Unmasking measures: Precision, Recall, C@1, F0.5u
\textcolor{violet}{
TBD: Show and analyze results\\
TBD: Explain possible problems using our approach (too much information is lost during transcription; Author's freedom on sub-segmental level is low; bag-of-words model in unmasking ignores much of the phonetic information)\\
}

%As indicated earlier, through cross validation, we can use the entire data set for training and for testing, thus milking it in the most effective way.

% Note run time for (transcription &) algorithms
% Explain OOF crossval (with diagram?)


% Note Gutenberg dataset is really small




% Discuss problem 1: text to phonemes is difficult and information is lost.
%As discussed above, converting speech to a detailed and accurate phonetic transcription is hard.
%Even more difficult is creating said transcription from text instead of speech.
%...
% Addendum to 1:
% Case 1: Author imparts phonetic preference unconsciously
% Case 2: Author makes effort to sound a certain way, e.g. reverent in Moby Dick -> Hides / Skews phonetic information
% Is there a way to amplify the first-case-features? If not then this doesn't make much sense.

% Discuss problem 2: We examine mostly sub-segmental features. Author freedom is small. Maybe supra-segmental features are better.
% ladefoged p278: phonetics of the individual [... are ...] difficult to describe een with spectrograms of the person's speech.
% Idea: Authors don't have much freedom with segmental features => segm. features are not as useful => Try supra-segmental features
% Also: For structures such as email addresses the freedom of choice is small. There are probably more things that are given by structures and thus give no author-specific information (e.g. syntactic rules).

% Addendum to 2: words that are similar or equal in their meaning, often sound similar (e.g. place names, chemicals, ...)
% And if they have they have different meanings, they most likely also have different lexical forms, rendering the need for a transcription obsolete.


% Problem 3 / Following thoughts: Phonetic transcriptions / features are probably bad Authorship predictors but better predictors for other features such as genre, topic, age of setting.