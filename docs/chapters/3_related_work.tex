\chapter{Related Work}\label{related_work}
% Maybe describe manual AA methods
% Describe other AA methods


Through the PAN workshops in 2013--15 and 2020 onward, numerous approaches aiming at solving Authorship Verification have been developed.
% Introduce PAN to embed other related work into context
% Other workshops?
\cite{stein2019unbiasedGutenbergCorpus} gives related work in Authorship Verification.

% Maybe put this after 4. Experiments, so readers don't have to wait?
% 1. Related work regarding non-phonological AI approaches
% - Unmasking approach: Forwards reference functionality, note significance
\cite{stein2019unbiasedGutenbergCorpus} reveals possible biases ($B1$--$B6$) in Authorship Verification and presents ways to mitigate these.
First, biases of AV algorithms are discussed.\\
$B1$: Models using corpus-relative features such as TF-IDF are prone to overfitting as in most cases the document frequencies are derived from the training set themselves.\\
$B2$: In a similar vein, models employing feature scaling also tend to overfit to the specifics of the training set.
Thus, care should be taken to avoid modelling the training data to closely.\\
Next, biases concerning the data are examined.\\
$B3$: A text may contain artifacts that were not introduced by the author, such as editorial marks or plain text conversion errors.
To prevent fitting to erroneous artifacts, texts should be fully homogenized to only contain artifacts entered by the author.\\
$B4$: To increase the size of a dataset, text chunks are often reused.
This should not be done, as the resulting corpus might over- or underrepresent certain authors' styles.\\
$B5$: Reusing text might lead to overlap including topic words, named entities and other unique character sequences.
To inhibit an AV algorithm learning these features, text overlap should be analyzed and corrected.\\
$B6$: Lastly, it is unrealistic for an AV algorithm to be used in situations where it has access to a large test set.
Therefore, while evaluating the algorithms should only have access to one text pair at a time.
This more closely models manual Authorship Verification where a forensic linguist also inspects text pairs on a case-by-case basis.\\
To mitigate the biases stemming from the data, a corpus containing texts from project Gutenberg is presented.
We will use this dataset in our experiments.


% Khomytska
\cite{khomytska2019nonparametric} analyzes the influence of eight different consonant phoneme classes in differentiating authorial style.
The consonant phoneme classes used group labial, velar, fricative, nasal, sonorous, coronal, dorsal, and stop phonemes.
First, a text pair is transcribed and then processed to yield a sample of 51,000 consonant phonemes for each text.
The sample is divided into 51 parts and the mean frequencies of the classes are calculated.
Using Pearson's test, it is proven that the obtained class frequencies follow a normal distribution.
To assess the similarity of the distributions, the Student's t-test, the Kolmogorov-Smirnov test, and the Chi-square test are examined.
Also, by comparing the phoneme class frequencies between the texts, differentiation-capabilities for each of the classes are determined.
It is concluded that labial, fricative, nasal, coronal, dorsal and  stop consonant phonemes in conjunction with the Kolmogorov-Smirnov test are useful for differentiating authorial style, whereas velar and sonorous consonant phonemes are not.
Khomytska and Teslyuk have published a number of very similar articles on Style Differentiation and Authorship Attribution\footnote{\url{https://scholar.google.com/citations?view_op=list_works&hl=en&hl=en&user=dnsPizAAAAAJ}}.
Unfortunately, no standard evaluation methods are used, preventing meaningful comparisons to other work in this area.
In addition, the used datasets are small, with the paper outlined above deriving a not further specified improvement in the differentiation of authorial styles by analyzing only one text pair, questioning the validity of the results.


%\\
% 2. phonetic approaches for other approaches
% phonological approaches
% -> Native language identification paper
Phonetic features have also been used in the task of Native Language Identification in \cite{smiley2017native}.
Given a text, the goal is to determine the native language of the author from a closed set of possible languages.
Labeled texts from a training set were transcribed using one of four algorithms.
Three of the algorithms used were Soundex, Double Metaphone and New York State Identification and Intelligence System.
Originally they were developed to improve recall in information retrieval systems when the exact spelling of a word is unknown.
Thus, they can be interpreted as broad transcription algorithms.
Also, text was transcribed using the Carnegie Mellon University Pronouncing Dictionary (CMU), resulting in a much narrower transcription.
After transcribing, the samples were segmented into character $n$-grams of sizes 2--9.
Then, the TF-IDF score for $n$-grams with a document frequency of at least 5 but not more than $5\%$ of the training set were calculated.
The scores were then used for training a linear C-Support Vector Machine.
Using only the features generated by the phonetic algorithms, the F1-score was worse then using plain character $n$-grams.
But when these features were combined with plain $n$-grams they increased the F1-score
Double Metaphone and plain $n$-grams resulted in the largest increase of $0.56\%$.
Also, it turned out that in all cases the broader transcriptions outperformed the narrow CMU transcription, except when using only Soundex features.
Thus, a transcription that is too narrow might increase the feature noise and damage the classifiers' performance.


% Khomytska