\chapter{Related Work}\label{related_work}

% Introduce PAN to embed other related work into context
% Other workshops?
\cite{stein2019unbiasedGutenbergCorpus} gives related work in Authorship Verification.

% Maybe put this after 4. Experiments, so readers don't have to wait?
% 1. Related work regarding non-phonological AI approaches
% - Unmasking approach: Forwards reference functionality, note significance
\cite{stein2019unbiasedGutenbergCorpus} reveals possible biases ($B1$--$B6$) in Authorship Verification and presents ways to mitigate these.
First, biases of AV algorithms are discussed.
$B1$: Models using corpus-relative features such as TF-IDF are prone to overfitting as in most cases the document frequencies are derived from the training set themselves.
$B2$: In a similar vein, models employing feature scaling also tend to overfit to the specifics of the training set.
Thus, care should be taken to avoid modelling the training data to closely.\\
Next, biases concerning the data are examined.
$B3$: A text may contain artifacts that were not introduced by the author, such as editorial marks or plain text conversion errors.
To prevent fitting to erroneous artifacts, texts should be fully homogenized to only contain artifacts entered by the author.
$B4$: To increase the size of a dataset, text chunks are often reused.
This should not be done, as the resulting corpus might over- or underrepresent certain authors' styles.
$B5$: Reusing text might lead to overlap including topic words, named entities and other unique character sequences.
To inhibit an AV algorithm learning these features, text overlap should be analyzed and corrected.\\
$B6$: Lastly, it is unrealistic for an AV algorithm to be used in situations where it has access to a large test set.
Therefore, while evaluating the algorithms should only have access to one text pair at a time.
This more closely models manual Authorship Verification where a forensic linguist also inspects text pairs on a case-by-case basis.\\
To mitigate the biases stemming from the data, a corpus containing texts from project Gutenberg is presented.
We will use this dataset in our experiments.

% 2. phonetic approaches for other approaches
% phonological approaches
% -> Native language identification paper
Phonetic features have been used in the task of Native Language Identification in \cite{smiley2017native}.
Given a text, the goal is to determine the native language of the author from a closed set of possible languages.
Labeled texts from a training set were transcribed using one of four algorithms.
Three of the algorithms used were Soundex, Double Metaphone and New York State Identification and Intelligence System.
Originally they were developed to improve recall in information retrieval systems when the exact spelling of a word is unknown.
Thus, they can be interpreted as broad transcription algorithms (word explained later...).
Also, text was transcribed using the Carnegie Mellon University Pronouncing Dictionary (CMU), resulting in a much narrower transcription.
After transcribing, the samples were segmented into character $n$-grams of sizes 2--9.
Then, the TF-IDF score for $n$-grams with a document frequency of at least 5 but not more than $5\%$ of the training set were calculated.
The scores were then used for training a linear C-Support Vector Machine.
Using only the features generated by the phonetic algorithms, the F1-score was worse then using plain character $n$-grams.
But when these features were combined with plain $n$-grams they increased the F1-score
Double Metaphone and plain $n$-grams resulted in the largest increase of $0.56\%$.
Also, it turned out that in all cases the broader transcriptions outperformed the narrow CMU transcription, except when using only Soundex features.
Thus, a transcription that is too narrow might increase the feature noise and damage the classifiers' performance.


% Khomytska