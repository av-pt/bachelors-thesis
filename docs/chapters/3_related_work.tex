\chapter{Related Work}\label{related_work}

% Introduce PAN to embed other related work into context
% Other workshops?

% Maybe put this after 4. Experiments, so readers don't have to wait?
% 1. Related work regarding non-phonological AI approaches
% - Unmasking approach: Forwards reference functionality, note significance
\cite{stein2019unbiasedGutenbergCorpus} gives related work in Authorship Verification.
-> Also list possible biases here! XXX Only keep if used later!\\
Model Bias\\
B1: Corpus-relative features, e.g. document frequency -> overfitting\\
B2: Feature scaling -> Overfitting towards corpus specifics\\
Data Bias\\
B3: Plain text heterogeneity, artifacts that are unlikely to signal authorial style,but rather originate from other sources, e.g. features like white spaces which vary across authors but were not necessarily introduced by them. Data sets should be fully homogenized.\\
B4: Population homogeneity, reusing chunks when creating the corpus might over- / underrepresent certain authors' styles.\\
B5: Accidental text overlap, named entities / topic words / repetitions / unique character sequences might give same author pairs away, so that algorithms learn these things instead of the wanted patterns.\\
Evaluation Bias\\
B6: Test conflation, verifiers can usually access the entire test dataset, this is not how a forensic linguist would do things -> test one case at a time.\\

% 2. phonetic approaches for other approaches
% phonological approaches
% -> Native language identification paper
Phonetic features have been used in the task of Native Language Identification in \cite{smiley2017native}.
Given a text, the goal is to determine the native language of the author from a closed set of possible languages.
Labeled texts from a training set were transcribed using one of four algorithms.
Three of the algorithms used were Soundex, Double Metaphone and New York State Identification and Intelligence System.
Originally they were developed to improve recall in information retrieval systems when the exact spelling of a word is unknown.
Thus, they can be interpreted as broad transcription algorithms (word explained later...).
Also, text was transcribed using the Carnegie Mellon University Pronouncing Dictionary (CMU), resulting in a much narrower transcription.
After transcribing, the samples were segmented into character $n$-grams of sizes 2--9.
Then, the TF-IDF score for $n$-grams with a document frequency of at least 5 but not more than $5\%$ of the training set were calculated.
The scores were then used for training a linear C-Support Vector Machine.
Using only the features generated by the phonetic algorithms, the F1-score was worse then using plain character $n$-grams.
But when these features were combined with plain $n$-grams they increased the F1-score
Double Metaphone and plain $n$-grams resulted in the largest increase of $0.56\%$.
Also, it turned out that in all cases the broader transcriptions outperformed the narrow CMU transcription, except when using only Soundex features.
Thus, a transcription that is too narrow might increase the feature noise and damage the classifiers' performance.