\chapter{Related Work}\label{related_work}
Through the PAN workshops in 2013--15 and 2020 onward, numerous approaches aiming at solving Authorship Verification have been developed.
% Introduce PAN to embed other related work into context
% Other workshops?
\cite{stein2019unbiasedGutenbergCorpus} gives related work in Authorship Verification.

% Maybe put this after 4. Experiments, so readers don't have to wait?
% 1. Related work regarding non-phonological AI approaches
% - Unmasking approach: Forwards reference functionality, note significance
\cite{stein2019unbiasedGutenbergCorpus} reveals possible biases ($B1$--$B6$) in Authorship Verification and presents ways to mitigate these.
First, biases of AV algorithms are discussed.\\
$B1$: Models using corpus-relative features such as TF-IDF are prone to overfitting as in most cases the document frequencies are derived from the training set themselves.\\
$B2$: In a similar vein, models employing feature scaling also tend to overfit to the specifics of the training set.
Thus, care should be taken to avoid modelling the training data to closely.\\
Next, biases concerning the data are examined.\\
$B3$: A text may contain artifacts that were not introduced by the author, such as editorial marks or plain text conversion errors.
To prevent fitting to erroneous artifacts, texts should be fully homogenized to only contain artifacts entered by the author.\\
$B4$: To increase the size of a dataset, text chunks are often reused.
This should not be done, as the resulting corpus might over- or underrepresent certain authors' styles.\\
$B5$: Reusing text might lead to overlap including topic words, named entities and other unique character sequences.
To inhibit an AV algorithm learning these features, text overlap should be analyzed and corrected.\\
$B6$: Lastly, it is unrealistic for an AV algorithm to be used in situations where it has access to a large test set.
Therefore, while evaluating the algorithms should only have access to one text pair at a time.
This more closely models manual Authorship Verification where a forensic linguist also inspects text pairs on a case-by-case basis.\\
To mitigate the biases stemming from the data, a corpus containing texts from project Gutenberg is presented.
We will use this dataset in our experiments.


% Khomytska
\cite{khomytska2018authorship} analyzes the influence of eight different consonant phoneme classes in differentiating between belles-lettres, colloquial and scientific styles of writing.
The consonant phoneme classes used were "labial, forelingual, mediolingual, backlingual, nasal, constrictive,occlusive and sonorant" \textcolor{teal}{(Verbatim quote. Are quotation marks needed here?)}.
First, text data was transcribed and then processed to yield a sample of 31,000 consonant phonemes.
The sample was divided into 31 parts and the mean frequencies of the classes was calculated.
By comparing the frequencies between texts of different styles, differentiation-capabilities for each of the phoneme classes have been determined.
It has been shown that the classes differ in their capability to serve for style differentiation based on the styles that are to be distinguished.
Motivated by these results, we use $Dolgo$ sound class $n$-grams in our research.
\textcolor{teal}{(Note: The above is the best I could do in understanding that paper, but I'm still unsure whether I actually understand it.
It reads as if it was computer-generated.
They also have papers directly on AV, but their research is really cryptic...)}

%\\
% 2. phonetic approaches for other approaches
% phonological approaches
% -> Native language identification paper
Phonetic features have also been used in the task of Native Language Identification in \cite{smiley2017native}.
Given a text, the goal is to determine the native language of the author from a closed set of possible languages.
Labeled texts from a training set were transcribed using one of four algorithms.
Three of the algorithms used were Soundex, Double Metaphone and New York State Identification and Intelligence System.
Originally they were developed to improve recall in information retrieval systems when the exact spelling of a word is unknown.
Thus, they can be interpreted as broad transcription algorithms.
Also, text was transcribed using the Carnegie Mellon University Pronouncing Dictionary (CMU), resulting in a much narrower transcription.
After transcribing, the samples were segmented into character $n$-grams of sizes 2--9.
Then, the TF-IDF score for $n$-grams with a document frequency of at least 5 but not more than $5\%$ of the training set were calculated.
The scores were then used for training a linear C-Support Vector Machine.
Using only the features generated by the phonetic algorithms, the F1-score was worse then using plain character $n$-grams.
But when these features were combined with plain $n$-grams they increased the F1-score
Double Metaphone and plain $n$-grams resulted in the largest increase of $0.56\%$.
Also, it turned out that in all cases the broader transcriptions outperformed the narrow CMU transcription, except when using only Soundex features.
Thus, a transcription that is too narrow might increase the feature noise and damage the classifiers' performance.


% Khomytska